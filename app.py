import re
import json
import time
import hashlib
from datetime import datetime
from urllib.parse import quote

import pandas as pd
import requests
import feedparser
import streamlit as st
from dateutil.relativedelta import relativedelta
from google import genai
from duckduckgo_search import DDGS  # [NEW] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€

# ============================================================
# KIHS (í•œêµ­ìˆ˜ìì›ì¡°ì‚¬ê¸°ìˆ ì›) ì˜¨ë¼ì¸ ë°ì´í„° ë¶„ì„ê¸° (Pro)
# - Sources: GDELT(ëŒ€ëŸ‰) + Google RSS(ì†ë³´) + DuckDuckGo(í•œê¸€ì •í™•ë„)
# - Quarter UI (ë¶„ê¸°/ë¸Œëœì¹˜)
# - Gemini ë¶„ì„ ë³´ê³ ì„œ Pool(ëˆ„ì  ì €ì¥)
# ============================================================

# -------------------------
# Page
# -------------------------
st.set_page_config(page_title="KIHS ì˜¨ë¼ì¸ ë°ì´í„° ë¶„ì„ê¸° (Pro)", layout="wide")
st.title("KIHS (í•œêµ­ìˆ˜ìì›ì¡°ì‚¬ê¸°ìˆ ì›)")
st.caption("ì˜¨ë¼ì¸ ë°ì´í„° ë¶„ì„ê¸° (Pro) â€” GDELT + DuckDuckGo + Google RSS + Gemini")

# -------------------------
# Secrets / Gemini
# -------------------------
api_key = st.secrets.get("GOOGLE_API_KEY")
if not api_key:
    st.error("âŒ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (Streamlit Cloud â†’ Settings â†’ Secrets)")
    st.stop()

try:
    client = genai.Client(api_key=api_key)
except Exception as e:
    st.error("âŒ Gemini Client ìƒì„± ì‹¤íŒ¨")
    st.exception(e)
    st.stop()

# -------------------------
# Session state (report pool)
# -------------------------
if "df" not in st.session_state:
    st.session_state["df"] = None
if "summary" not in st.session_state:
    st.session_state["summary"] = None
if "quarters" not in st.session_state:
    st.session_state["quarters"] = []
if "report_pool" not in st.session_state:
    st.session_state["report_pool"] = {}

# -------------------------
# Quarter utilities
# -------------------------
def parse_quarter(qstr: str):
    # "2024-Q1" -> (start, end_exclusive)
    y, q = qstr.split("-Q")
    y, q = int(y), int(q)
    start_month = (q - 1) * 3 + 1
    start = datetime(y, start_month, 1)
    end = start + relativedelta(months=3)
    return start, end

def quarter_label(dt: datetime) -> str:
    q = (dt.month - 1) // 3 + 1
    return f"{dt.year}-Q{q}"

def quarter_iter(start_q: str, end_q: str):
    # inclusive labels
    s, _ = parse_quarter(start_q)
    _, end_excl = parse_quarter(end_q)
    cur = s
    while cur < end_excl:
        qlab = quarter_label(cur)
        nxt = cur + relativedelta(months=3)
        yield qlab, cur, nxt
        cur = nxt

def dt_to_gdelt(dt: datetime) -> str:
    return dt.strftime("%Y%m%d%H%M%S")

def safe_iso_from_gdelt(seendate: str):
    # "YYYYMMDDHHMMSS" -> iso
    if isinstance(seendate, str) and re.fullmatch(r"\d{14}", seendate):
        try:
            return datetime.strptime(seendate, "%Y%m%d%H%M%S").isoformat()
        except Exception:
            return None
    return None

# -------------------------
# Collectors (cached)
# -------------------------
@st.cache_data(show_spinner=False, ttl=60 * 30)
def fetch_gdelt_doc(query: str, start_dt: datetime, end_dt: datetime, max_records: int = 250):
    base = "https://api.gdeltproject.org/api/v2/doc/doc"
    out = []
    startrecord = 0  # GDELT API ì¡°ì •
    pagesize = min(250, max_records)

    # í•œê¸€ ê²€ìƒ‰ íŒ: GDELTëŠ” í•œêµ­ì–´ ì¿¼ë¦¬ê°€ ì•½í•˜ë¯€ë¡œ sourcecountry:KS ì¶”ê°€ ê³ ë ¤ ê°€ëŠ¥í•˜ë‚˜
    # ì—¬ê¸°ì„œëŠ” ì›ë³¸ ë¡œì§ ìœ ì§€í•˜ë˜ ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
    
    while len(out) < max_records:
        params = {
            "query": query,
            "mode": "artlist",
            "format": "json",
            "startdatetime": dt_to_gdelt(start_dt),
            "enddatetime": dt_to_gdelt(end_dt),
            "maxrecords": min(pagesize, max_records - len(out)),
            "startrecord": startrecord,
            "sort": "datedesc",
        }
        try:
            r = requests.get(base, params=params, timeout=10)
            if r.status_code != 200:
                break
            data = r.json()
        except Exception:
            break

        articles = data.get("articles") or []
        if not articles:
            break

        for a in articles:
            out.append(
                {
                    "source_system": "GDELT",
                    "title": a.get("title"),
                    "url": a.get("url"),
                    "domain": a.get("domain"),
                    "language": a.get("language"),
                    "published": safe_iso_from_gdelt(a.get("seendate")),
                    "snippet": a.get("snippet", ""),
                    "source": a.get("domain"),
                }
            )

        fetched = len(articles)
        startrecord += fetched
        if fetched < params.get("maxrecords", 250):
            break

        time.sleep(0.15) 

    return out

@st.cache_data(show_spinner=False, ttl=60 * 30)
def fetch_google_news_rss(query: str, hl="ko", gl="KR", ceid="KR:ko", limit=80):
    q = quote(query)
    url = f"https://news.google.com/rss/search?q={q}&hl={hl}&gl={gl}&ceid={ceid}"
    d = feedparser.parse(url)

    out = []
    for e in (d.entries or [])[:limit]:
        published_iso = None
        if hasattr(e, "published_parsed") and e.published_parsed:
            try:
                published_iso = datetime(*e.published_parsed[:6]).isoformat()
            except Exception:
                published_iso = None

        src = None
        if hasattr(e, "source"):
            try:
                src = e.source.get("title")
            except Exception:
                src = None

        out.append(
            {
                "source_system": "GoogleRSS",
                "title": getattr(e, "title", None),
                "url": getattr(e, "link", None),
                "domain": "news.google.com",
                "language": "ko",
                "published": published_iso,
                "snippet": getattr(e, "summary", "") or "",
                "source": src,
            }
        )
    return out

@st.cache_data(show_spinner=False, ttl=60 * 30)
def fetch_duckduckgo_news(query: str, max_results: int = 50):
    """
    [NEW] DuckDuckGo ìˆ˜ì§‘ê¸° ì¶”ê°€
    í•œêµ­ì–´ í‚¤ì›Œë“œì— ë§¤ìš° ê°•ë ¥í•˜ë©°, ë³¸ë¬¸ ìš”ì•½(snippet) í’ˆì§ˆì´ ì¢‹ìŠµë‹ˆë‹¤.
    """
    out = []
    try:
        with DDGS() as ddgs:
            # region='kr-kr'ë¡œ í•œêµ­ ì–¸ë¡ ì‚¬ ìš°ì„  ê²€ìƒ‰
            ddg_gen = ddgs.news(
                keywords=query,
                region="kr-kr",
                safesearch="off",
                timelimit="y",  # ìµœê·¼ 1ë…„ì¹˜ ê²€ìƒ‰ (ì´í›„ ë¡œì§ì—ì„œ ë¶„ê¸°ë³„ë¡œ í•„í„°ë§ë¨)
                max_results=max_results
            )
            
            for r in ddg_gen:
                # ë‚ ì§œ íŒŒì‹± ì‹œë„ (DDGëŠ” ISO ë¹„ìŠ·í•œ í¬ë§·ìœ¼ë¡œ ì¤Œ)
                pub_iso = None
                raw_date = r.get('date')
                if raw_date:
                    try:
                        # 2024-05-20T14:00:00+00:00 í˜•ì‹ì´ ì¼ë°˜ì 
                        dt = datetime.fromisoformat(raw_date.replace('Z', '+00:00'))
                        pub_iso = dt.isoformat()
                    except:
                        pub_iso = None # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ì‹œ None ì²˜ë¦¬ (ì´í›„ ë¡œì§ì—ì„œ ê±¸ëŸ¬ì§)

                out.append({
                    "source_system": "DuckDuckGo",
                    "title": r.get('title'),
                    "url": r.get('url'),
                    "domain": r.get('source'), # DDGëŠ” source í•„ë“œì— ì–¸ë¡ ì‚¬ëª… ì œê³µ
                    "language": "ko",
                    "published": pub_iso,
                    "snippet": r.get('body', ''),
                    "source": r.get('source'),
                })
    except Exception as e:
        # st.error(f"DDG Error: {e}") # ì‚¬ìš©ìì—ê²Œ ì—ëŸ¬ ë…¸ì¶œ ìµœì†Œí™”
        pass
        
    return out

# -------------------------
# Normalization / dedup / sentiment
# -------------------------
POS_WORDS = ["í™•ëŒ€", "ì„±ì¥", "ë„ì…", "ê°œì„ ", "ì„±ê³¼", "í˜ì‹ ", "ì§€ì›", "íˆ¬ì", "ìƒìš©í™”", "ì„±ê³µ", "í˜‘ë ¥", "ë°œì „", "ì²´ê²°", "ë‹¬ì„±"]
NEG_WORDS = ["ìš°ë ¤", "ë…¼ë€", "ì‹¤íŒ¨", "ì¤‘ë‹¨", "ê·œì œ", "ì‚¬ê³ ", "ë¶€ì¡±", "ì§€ì—°", "ìœ„í—˜", "ê°ˆë“±", "ë°˜ëŒ€", "í”¼í•´", "ì ë°œ", "ì˜¤ì—¼"]

def rule_sentiment(text: str) -> str:
    t = (text or "").lower()
    p = sum(w in t for w in POS_WORDS)
    n = sum(w in t for w in NEG_WORDS)
    if p > n:
        return "ê¸ì •"
    if n > p:
        return "ë¶€ì •"
    return "ì¤‘ë¦½"

def make_key(url: str, title: str) -> str:
    base = (url or "").strip() or (title or "").strip()
    return hashlib.md5(base.encode("utf-8", errors="ignore")).hexdigest()

def query_fingerprint(query: str, start_q: str, end_q: str, use_gdelt: bool, use_rss: bool, use_ddg: bool, gdelt_max: int, rss_max: int, ddg_max: int):
    s = json.dumps(
        {
            "query": query,
            "start_q": start_q,
            "end_q": end_q,
            "use_gdelt": use_gdelt,
            "use_rss": use_rss,
            "use_ddg": use_ddg,
            "gdelt_max": gdelt_max,
            "rss_max": rss_max,
            "ddg_max": ddg_max
        },
        ensure_ascii=False,
        sort_keys=True,
    )
    return hashlib.md5(s.encode("utf-8")).hexdigest()

# -------------------------
# Gemini reporting
# -------------------------
def build_quarter_bullets(dfq: pd.DataFrame, cap: int = 150) -> str:
    d = dfq.copy()
    d["published_dt"] = pd.to_datetime(d["published"], errors="coerce")
    d = d.sort_values("published_dt", ascending=False).drop(columns=["published_dt"])

    lines = []
    for _, r in d.head(cap).iterrows():
        # Source Systemì„ ì•ì— í‘œê¸°í•˜ì—¬ ì¶œì²˜ êµ¬ë¶„
        src = r.get("domain") or r.get("source") or "Unknown"
        lines.append(f"- [{r['source_system']}/{r['sentiment']}] {r['title']} ({src})")
    return "\n".join(lines)

def gemini_report(quarter: str, bullets: str, model_name: str):
    prompt = f"""
ë‹¹ì‹ ì€ 'KIHS (í•œêµ­ìˆ˜ìì›ì¡°ì‚¬ê¸°ìˆ ì›) ì§€ëŠ¥í˜• ë°ì´í„° ë¶„ì„ê¸°'ì˜ ìˆ˜ì„ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ëŒ€ìƒ ë¶„ê¸°: {quarter}

ì•„ë˜ëŠ” GDELT(ê¸€ë¡œë²Œ), DuckDuckGo(í•œêµ­ì–´), Google RSSì—ì„œ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì…ë‹ˆë‹¤.
ì´ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ ê³¼ì¥ ì—†ì´ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ë™í–¥ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

[ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°]
{bullets}

[ë³´ê³ ì„œ ì‘ì„± ì–‘ì‹]
1. **ë¶„ê¸° ì´í‰ ë° í•µì‹¬ ì´ìŠˆ** (3~5ë¬¸ì¥ ìš”ì•½)
2. **ì£¼ìš” ê¸°ê´€/ê¸°ì—… ë™í–¥** - KIHS, K-water, í™˜ê²½ë¶€ ë“± ì£¼ìš” ì£¼ì²´ë³„ í™œë™ ì •ë¦¬
3. **ë¶„ì•¼ë³„ ì´ìŠˆ ë¶„ì„** (ê¸ì •/ë¶€ì •)
   - [ê¸ì •/ì„±ê³¼] ê¸°ìˆ  ë„ì…, í˜‘ì•½ ì²´ê²° ë“±
   - [ë¶€ì •/ë¦¬ìŠ¤í¬] ê°€ë­„/í™ìˆ˜ í”¼í•´, ê°ˆë“±, ì‚¬ê³  ë“±
4. **KIHSë¥¼ ìœ„í•œ ì‹œì‚¬ì ** (ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì–¸ 3ê°€ì§€)
5. **ì°¨ê¸° ëª¨ë‹ˆí„°ë§ í‚¤ì›Œë“œ** (í•´ì‹œíƒœê·¸ 5ê°œ)

ì œì•½ì‚¬í•­:
- ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ ê²ƒ.
- 'DuckDuckGo', 'GDELT' ë“±ì˜ ì‹œìŠ¤í…œ ìš©ì–´ëŠ” ë³´ê³ ì„œ ë³¸ë¬¸ì— ì“°ì§€ ë§ ê²ƒ.
- ë¬¸ì²´ëŠ” "í•¨.", "ë¨." ë“±ì˜ ê°œì¡°ì‹ì´ ì•„ë‹Œ, ì •ì¤‘í•œ ë³´ê³ ì„œì²´("í•˜ì˜€ìŠµë‹ˆë‹¤.")ë¥¼ ì‚¬ìš©í•  ê²ƒ.
"""
    res = client.models.generate_content(model=model_name, contents=prompt)
    return res.text or ""

# -------------------------
# Sidebar UI
# -------------------------
with st.sidebar:
    st.header("ì„¤ì •")

    st.subheader("ê¸°ê°„(ë¶„ê¸°) ì„ íƒ")
    c1, c2 = st.columns(2)
    with c1:
        start_q = st.text_input("ì‹œì‘ ë¶„ê¸°", value="2024-Q1")
    with c2:
        end_q = st.text_input("ì¢…ë£Œ ë¶„ê¸°", value="2025-Q1")

    st.subheader("ê²€ìƒ‰ì–´(Query)")
    # í•œêµ­ì–´ ê²€ìƒ‰ì„ ìœ„í•´ í‚¤ì›Œë“œ ë³´ê°•
    default_query = (
        '"KIHS" OR "í•œêµ­ìˆ˜ìì›ì¡°ì‚¬ê¸°ìˆ ì›" OR "ìˆ˜ìì›ê³µì‚¬" OR "í™˜ê²½ë¶€ ë¬¼ê´€ë¦¬" OR '
        'flood OR drought OR "smart water" OR "digital twin"'
    )
    query = st.text_area("ê²€ìƒ‰ì–´", value=default_query, height=120)

    st.subheader("ë°ì´í„° ì†ŒìŠ¤ ì„¤ì •")
    
    # 1. DuckDuckGo (New, Korean Strong)
    use_ddg = st.checkbox("DuckDuckGo (í•œêµ­ì–´ ì¶”ì²œ)", value=True, help="í•œêµ­ì–´ ë‰´ìŠ¤ ê²€ìƒ‰ ì •í™•ë„ê°€ ë†’ìŠµë‹ˆë‹¤.")
    ddg_max = st.slider("DDG ìˆ˜ì§‘ëŸ‰ (ì†ŒìŠ¤ë‹¹)", 30, 200, 100, 10)
    
    # 2. RSS (Supplement)
    use_rss = st.checkbox("Google News RSS (ë³´ê°•ìš©)", value=True)
    rss_max = st.slider("RSS ìˆ˜ì§‘ëŸ‰ (ì „ì²´)", 20, 200, 80, 10)

    # 3. GDELT (Global, BigData)
    use_gdelt = st.checkbox("GDELT (í•´ì™¸/ë¹…ë°ì´í„°)", value=True, help="í•œê¸€ ê²€ìƒ‰ì€ ì•½í•˜ì§€ë§Œ, ì˜ë¬¸/ê¸€ë¡œë²Œ ì¶”ì„¸ íŒŒì•…ì— í•„ìˆ˜ì ì…ë‹ˆë‹¤.")
    gdelt_max = st.slider("GDELT ìˆ˜ì§‘ëŸ‰ (ë¶„ê¸°ë‹¹)", 50, 1000, 250, 50)
    
    st.markdown("---")
    st.subheader("LLM ì„¤ì •")
    model_name = st.selectbox("ëª¨ë¸", ["gemini-2.5-flash", "gemini-1.5-pro"], index=0)

    st.markdown("---")
    btn_collect = st.button("â‘  ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬", type="primary")
    btn_analyze = st.button("â‘¡ AI ë³´ê³ ì„œ ìƒì„±", type="secondary")
    btn_clear_pool = st.button("ì´ˆê¸°í™” (Reset)", type="tertiary")

# -------------------------
# Actions
# -------------------------
if btn_clear_pool:
    st.session_state["report_pool"] = {}
    st.session_state["df"] = None
    st.session_state["summary"] = None
    st.success("ëª¨ë“  ë°ì´í„°ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")

def run_collection():
    # validate quarters
    try:
        quarters = list(quarter_iter(start_q, end_q))
        if not quarters:
            raise ValueError("empty")
    except Exception:
        st.error("ë¶„ê¸° í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ì˜ˆ: 2024-Q1")
        return

    all_rows = []
    
    # 1. Collect GDELT (ë¶„ê¸°ë³„ ë£¨í”„)
    if use_gdelt:
        with st.spinner("ğŸŒ GDELT(ê¸€ë¡œë²Œ) ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
            for qlab, qs, qe in quarters:
                recs = fetch_gdelt_doc(query, qs, qe, max_records=gdelt_max)
                for r in recs:
                    r["quarter"] = qlab
                    all_rows.append(r)

    # 2. Collect RSS (ì „ì²´ ê¸°ê°„ -> ë‚ ì§œê¸°ë°˜ í• ë‹¹)
    if use_rss:
        with st.spinner("ğŸ“° Google News RSS ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
            rss_recs = fetch_google_news_rss(query, limit=rss_max)
            for r in rss_recs:
                pub = r.get("published")
                if not pub: continue
                try:
                    dt = datetime.fromisoformat(pub)
                    r["quarter"] = quarter_label(dt)
                    all_rows.append(r)
                except:
                    continue

    # 3. Collect DuckDuckGo (ì „ì²´ ê¸°ê°„ -> ë‚ ì§œê¸°ë°˜ í• ë‹¹) [NEW]
    if use_ddg:
        with st.spinner("ğŸ¦† DuckDuckGo(í•œêµ­ì–´) ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
            ddg_recs = fetch_duckduckgo_news(query, max_results=ddg_max)
            for r in ddg_recs:
                pub = r.get("published")
                if not pub: continue
                try:
                    dt = datetime.fromisoformat(pub)
                    r["quarter"] = quarter_label(dt)
                    all_rows.append(r)
                except:
                    # ë‚ ì§œ í˜•ì‹ì´ ì•ˆ ë§ìœ¼ë©´ í˜„ì¬ ë¶„ê¸° í˜¹ì€ ì œì™¸ ì²˜ë¦¬
                    continue

    if not all_rows:
        st.warning("ìˆ˜ì§‘ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´/ê¸°ê°„/ì†ŒìŠ¤ë¥¼ ì¡°ì •í•´ ì£¼ì„¸ìš”.")
        return

    df = pd.DataFrame(all_rows)

    # normalize cols
    needed = ["title", "url", "published", "source_system", "quarter", "domain", "language", "snippet", "source"]
    for c in needed:
        if c not in df.columns:
            df[c] = None

    # dedup
    df["key"] = [make_key(u, t) for u, t in zip(df["url"].astype(str), df["title"].astype(str))]
    df = df.drop_duplicates(subset=["key"]).copy()

    # strictly filter quarters within selected range
    # (RSSì™€ DDGëŠ” ìµœê·¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ë¯€ë¡œ, ì‚¬ìš©ìê°€ ì„ íƒí•œ ë¶„ê¸° ë²”ìœ„ë¥¼ ë²—ì–´ë‚  ìˆ˜ ìˆìŒ -> í•„í„°ë§)
    wanted_quarters = [q for q, _, _ in quarters]
    df = df[df["quarter"].isin(wanted_quarters)].copy()

    # sentiment
    df["sentiment"] = df["title"].apply(rule_sentiment)

    # sort
    df["published_dt"] = pd.to_datetime(df["published"], errors="coerce")
    df = df.sort_values(["quarter", "published_dt"], ascending=[True, False]).drop(columns=["published_dt"])

    # summary table
    summary = (
        df.groupby(["quarter", "sentiment"])
        .size()
        .reset_index(name="count")
        .pivot(index="quarter", columns="sentiment", values="count")
        .fillna(0)
        .astype(int)
        .reset_index()
    )

    st.session_state["df"] = df
    st.session_state["summary"] = summary
    st.session_state["quarters"] = wanted_quarters

    st.success(f"âœ… ìˆ˜ì§‘ ë° í†µí•© ì™„ë£Œ: ì´ {len(df):,}ê±´ (GDELT+RSS+DDG)")

if btn_collect:
    run_collection()

# -------------------------
# Main view
# -------------------------
df = st.session_state.get("df")
summary = st.session_state.get("summary")
quarters = st.session_state.get("quarters", [])
report_pool = st.session_state.get("report_pool", {})

if df is None or summary is None:
    st.info("ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ **â‘  ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬** ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    st.stop()

# Layout
left, right = st.columns([1.3, 1.0], gap="large")

with left:
    st.subheader("ğŸ—‚ï¸ ìˆ˜ì§‘ ë°ì´í„° í•„í„°ë§")
    st.write(f"í†µí•© ë°ì´í„°ë² ì´ìŠ¤: **{len(df):,}ê±´**")

    f1, f2, f3, f4 = st.columns(4)
    with f1:
        quarter_sel = st.selectbox("ë¶„ê¸° ì„ íƒ", sorted(df["quarter"].unique()))
    with f2:
        source_sel = st.selectbox("ì¶œì²˜", ["ì „ì²´"] + sorted(list(df["source_system"].dropna().unique())))
    with f3:
        sentiment_sel = st.selectbox("ê°ì„±", ["ì „ì²´", "ê¸ì •", "ì¤‘ë¦½", "ë¶€ì •"], index=0)
    with f4:
        kw = st.text_input("ë‚´ìš© ê²€ìƒ‰", value="", placeholder="í‚¤ì›Œë“œ ì…ë ¥")

    # Filter Logic
    dff = df[df["quarter"] == quarter_sel].copy()
    if source_sel != "ì „ì²´":
        dff = dff[dff["source_system"] == source_sel]
    if sentiment_sel != "ì „ì²´":
        dff = dff[dff["sentiment"] == sentiment_sel]
    if kw.strip():
        mask = dff["title"].str.contains(kw, case=False, na=False) | dff["snippet"].str.contains(kw, case=False, na=False)
        dff = dff[mask]

    show_cols = ["published", "source_system", "sentiment", "title", "domain", "url"]
    st.dataframe(dff[show_cols], use_container_width=True, height=450, hide_index=True)

    csv_bytes = dff[show_cols].to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    st.download_button(
        label="ğŸ“¥ í˜„ì¬ í•„í„° ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
        data=csv_bytes,
        file_name=f"KIHS_{quarter_sel}_filtered.csv",
        mime="text/csv",
    )

with right:
    st.subheader("ğŸ“Š ë¶„ê¸°ë³„ ë°ì´í„° ìš”ì•½")
    st.dataframe(summary, use_container_width=True, height=150, hide_index=True)

    chart_df = summary.set_index("quarter")
    for col in ["ê¸ì •", "ì¤‘ë¦½", "ë¶€ì •"]:
        if col not in chart_df.columns:
            chart_df[col] = 0
    st.bar_chart(chart_df[["ê¸ì •", "ì¤‘ë¦½", "ë¶€ì •"]], height=200, stack=True)

    st.markdown("---")
    st.subheader("ğŸ“ AI ë¶„ì„ ë¦¬í¬íŠ¸ (Report Pool)")
    
    # pool overview
    if report_pool:
        pool_list = []
        for q, meta in report_pool.items():
            pool_list.append({
                "ë¶„ê¸°": q,
                "ì‘ì„±ì‹œê°": meta.get("created_at", "").split("T")[1][:5], # ì‹œê°„ë§Œ í‘œì‹œ
                "ê¸°ì‚¬ìˆ˜": meta.get("n_items", 0),
            })
        st.dataframe(pd.DataFrame(pool_list).sort_values("ë¶„ê¸°"), use_container_width=True, height=120, hide_index=True)
    else:
        st.info("ìƒì„±ëœ ë³´ê³ ì„œê°€ ì—†ìŠµë‹ˆë‹¤. í•˜ë‹¨ì—ì„œ ìƒì„±í•˜ì„¸ìš”.")

    st.markdown("#### ë³´ê³ ì„œ ìƒì„± ë° ì—´ëŒ")
    gen_targets = st.multiselect("ë¶„ì„ ëŒ€ìƒ ë¶„ê¸° ì„ íƒ", quarters_list, default=[quarter_sel])

    # Analyze action
    if btn_analyze:
        # Use simple hash for demo
        fp = query_fingerprint(query, start_q, end_q, use_gdelt, use_rss, use_ddg, gdelt_max, rss_max, ddg_max)
        
        with st.spinner("Geminiê°€ ë°ì´í„°ë¥¼ ì½ê³  ë³´ê³ ì„œë¥¼ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
            new_pool = dict(report_pool)
            for q in gen_targets:
                dfq = df[df["quarter"] == q]
                if dfq.empty:
                    continue
                    
                # í”„ë¡¬í”„íŠ¸ì— ë“¤ì–´ê°ˆ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ìƒì„±
                bullets = build_quarter_bullets(dfq, cap=100) # í† í° ì œí•œ ê³ ë ¤
                
                try:
                    text = gemini_report(q, bullets, model_name=model_name)
                    new_pool[q] = {
                        "created_at": datetime.now().isoformat(timespec="seconds"),
                        "model": model_name,
                        "n_items": int(len(dfq)),
                        "query_hash": fp,
                        "text": text,
                    }
                except Exception as e:
                    st.error(f"{q} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            
            st.session_state["report_pool"] = new_pool
            report_pool = new_pool
        st.success("ë³´ê³ ì„œ ì‘ì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    # View report
    if report_pool:
        view_q = st.selectbox("ì—´ëŒí•  ë³´ê³ ì„œ ë¶„ê¸°", options=sorted(report_pool.keys()), key="view_q")
        with st.expander(f"{view_q} ë³´ê³ ì„œ ë³´ê¸°", expanded=True):
            st.markdown(report_pool[view_q].get("text", ""))

        st.download_button(
            label="ğŸ“„ ì „ì²´ ë¦¬í¬íŠ¸ í’€(JSON) ì €ì¥",
            data=json.dumps(report_pool, ensure_ascii=False, indent=2).encode("utf-8"),
            file_name="KIHS_report_pool.json",
            mime="application/json",
        )
